{\rtf1\ansi\ansicpg1252\cocoartf2636
\cocoatextscaling0\cocoaplatform0{\fonttbl\f0\fswiss\fcharset0 Helvetica;\f1\fswiss\fcharset0 Helvetica-Bold;\f2\fswiss\fcharset0 Helvetica-Oblique;
}
{\colortbl;\red255\green255\blue255;\red251\green2\blue7;\red0\green0\blue255;\red0\green0\blue0;
\red0\green0\blue0;\red0\green0\blue255;\red251\green2\blue7;}
{\*\expandedcolortbl;;\cssrgb\c100000\c14913\c0;\cssrgb\c1680\c19835\c100000;\cssrgb\c0\c0\c0;
\csgenericrgb\c0\c0\c0;\csgenericrgb\c0\c0\c100000;\cssrgb\c100000\c14913\c0;}
\margl1440\margr1440\vieww28300\viewh14040\viewkind0
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0

\f0\fs48 \cf0 #\
# \
#\
# 
\f1\b 9/30/23
\f0\b0  \cf2 \ul \ulc2 Generic R script\cf0 \ulnone  for implementing Bougnol&Dula Recommender Problem with Convex Hulls (CH)  and Linear \
# Programming (LP) to generate predictions for nUsers.\
# The script will also generate the user/item predictions using Pearson\'92s correlations\
#\
# Input: the utility matrix with \cf3 nItems \cf0  rows for the items (e.g. Movies, Jokes, Artists, etc.) and \cf3 nUsers\cf0  columns with the users (or the transpose). \
# Note: we assume the matrixes have column headings and row labels. \
# Note: the matrix may have been stored as its transpose in an Excel file. In this case store in \cf3 utilMtxT\cf0 . \
# Note: The order of the items should be shuffled.\
# Output: Two matrixes, \cf3 PredictPearsonAll \cf0 and \cf3 PredictCHullsAll,\cf4  with dimension\cf3  \cf4 (\cf3 nUsers X nItems\cf4 ) \cf0 containing the predicted ratings for the items in the \'93validation\'94 (or \'93test\'94) submatrix: one using Pearson\'92s correlations\cf3  \cf0  and the other using convex hulls and LPs.\
#\
# Parameters: (\cf3 R\cf0  variable in \cf3 blue\cf0 )\
#	Number of Users: \cf3 nUsers\cf0 \
#	Number of Items: \cf3 nItems\cf0 \
#	Number of items in the training set: \cf3 nTrain\cf0   (< \cf3 nItems\cf4 )\cf0 \
#\
# Utility and other matrixes:\
#	\cf3 utilMtx\cf0 : utility matrix with dimension \cf3 nItems\cf0  X \cf3 nUsers\cf0 .\
#	\cf3 utilMtxT\cf0 : Transpose of utility matrix (dimension: \cf3 nUsers\cf0   X \cf3 nItems\cf0 ). Necessary is Excel\'92s problems with column limits when \cf3 nUsers\cf0   >> \cf3 nItems \cf0 .\
#	\cf3 trainMtx: \cf0 Training set with dimension \cf3 nTrain\cf0  X \cf3 nUsers\cf0 \
#	\
# ToC\
# Part 1: Prepare the environment (working directory, load packages,  etc.).\
# Part 2:  Load \cf3 utilMtx  (\cf0 or  \cf3 utilMtxT)\cf0  from an Excel file worksheet.\
# Part 3: Extract training submatrix (\cf3 trainMtx\cf0 ) with dimension \cf3 nTrain\cf0  X \cf3 nUsers\cf0 .\
# Part 4: Create the matrix, \cf3 AllPearsCorr\cf0 , of all Pearson\'92s correlations using \cf3 trainMtx\cf0 .\
# Part 5: Create the matrix, \cf3 AllLPsols\cf0 , with the LP solutions for each user with convex hulls and LPs using \cf3 trainMtx\cf0 .\
# Part 6: Generate the predictions on the validations (\'93test\'94) data using the trained models one with Pearson\'92s and the other with Convex Hulls.\
\
\
\
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0
\cf0 \ul # Part 1: Prepare the environment\ulnone \
# Step 1: Change working directory to the location of the utility matrix in the form of Items as rows and users as columns: in OSX: Use \'93Misc\'94 tab. 
\f2\i The script assumes the data is in an Excel sheet and has column headings and row labels
\f0\i0 .\
# Step 2: Load the required libraries: Use 
\f2\i \ul Package Installer
\f0\i0 \ulnone  in tab \'93Packages & Data\'94: \cf3 gdata\cf0 () to read Excel worksheet containing the utility matrix, and \cf3 lpSolve\cf0 ().\
# \
# \ul Part 2: Load the utility matrix from a worksheet in an Excel file (make sure =gdata() is loaded (use \'93=library(gdata)\'94 to check)\
#\ulnone \
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0
\cf4 # Set names of the Excel file and the worksheet\
#\
\cf3 > fileName = "CrowdFlower100X893.xlsx" \cf4 #Name of Excel file containing the utility matrix.\
\cf3 > sheetName = "Top100Tracks100X893"   \cf4 #Name of Worksheet in the Excel file.\cf3 \
> utilMtx = read.xls(fileName, sheet=sheetName, header=TRUE, row.names=1) \cf4 #Data set has columns headings and row labels.\
\
#\
# Keep the transpose of the utility matrix since it will be used in a couple of places\
#\
\cf3 > utilMtxT=t(utilMtx)\cf0 \
\cf4 \
#\
# Place dimensions of utility matrix into variables \cf3 nUsers & nItems:\cf4 \
#\cf3 \
> utilMtxDim=dim(utilMtx)\
> nItems=utilMtxDim[1]\
> nUsers=utilMtxDim[2]\cf4 \
#\cf0 \
# \ul Part 3: Extract training set from the utility matrix.\ulnone \
#\cf2 \
\cf4 #\
# Set cardinality of training set, \cf3 nTrain\cf4 , using a strict subset of \cf3 nItems:\
\cf4 # Assume use the first\cf3  nTrain \cf4 items\
#\
\cf3 >nTrain=floor(nItems/2) \cf4 #Must be an integer! or\
\cf3 >nTrain=???\cf2 \
\cf4 >\cf3 trainMtx=utilMtx[1:nTrain,] \cf4 #Extract \cf3 nTrain\cf4  rows (items; assuming items in full utility matrix have been shuffled).\
\cf0 \
#\cf2 \
\cf4 #\cf0 \
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0
\cf0 \ul Part 4: Create the matrix of all Pearson\'92s correlations.\
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0
\cf0 \ulnone #\cf2 \
\cf4 #\cf0 \
> \cf3 AllPearsCorr <- cor(trainMtx, trainMtx, method="pearson")  \cf4 # Read response to warning about zero std. dev.\cf3 \
\cf0 #\cf2 \
\cf4 # If \'85\cf2 \
\cf4 # \cf2 Warning message: In cor(\cf3 trainMtx\cf2 , \cf3 trainMtx\cf2 ,  : the standard deviation is zero.\
\cf4 # \
# This warning means that there are duplicate users (columns with dimension \cf3 nTrain\cf4 ) in the training set (usually because all zeroes). \
# Remove duplicate users. If problem persist, look for duplicate non-zero columns.\
# Problem may be solved if the users (rows) in the full utility matrix, \cf3 utilMtx\cf4 , are reshuffled (since we are extracting a subset for training).\cf2  \
\cf4 # 
\f2\i This problem must be resolved otherwise the process will fail.
\f0\i0 \cf2 \
\cf4 #\
# Pearson\'92s correlations with timer (single-line command):\
#\cf2 \
\cf3 > start_time <- Sys.time();  \
AllPearsCorr <- cor(trainMtx, trainMtx, method="pearson");\
end_time <- Sys.time();\
Pearsontime=end_time - start_time; Pearsontime\cf2 \
Time difference of ??? secs\
\cf0 \
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0
\cf0 \ul Part 5: Find neighbors using convex hulls with LPs. Use lpSolve()\ulnone \
> r\cf3 equire(lpSolve)\cf0 \
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0
\cf2 Loading required package: lpSolve\cf0 \
> \cf3 library(lpSolve)\cf0 \
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0
\cf5 #\
# Translate the data in preparation for gauge LP.\
# Recall that the data is translated so the barycenter becomes the origin.\
# \
> \cf3 TranslateDataForLP = t(apply(trainMtx, 1, scale, scale=FALSE, center=TRUE))\cf5 \
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0
\cf2 \
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0
\cf4 #\
# Prepare the variables used in lpSolve(): OF coefficients and sense of the constraints.\
#\
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0
\cf6 > rp3.obj = rep(1, nUsers)		\cf4 #LPs\'92 Objective Function coefficients are all 1s; \cf6 \
> rp3.dir = rep("=", nTrain) \cf4   # \cf3 nTrain\cf4  constraints are equalities.\
#\
# \cf5 Initialize matrix where BFSs for all users using the items in the training set will be dumped.\
# Each of the \cf3 nUsers\cf5  will have \cf3 nTrain\cf5  neighbors because each BFS has \cf3 nTrain\cf5  basic variables.\
# These are the extreme points of the projected facet; i.e., convex hull neighbors.\
# They will be compared with those using Pearson\'92s correlation (in variable \'93AllPearsCorr\'94).\cf4 \
#\
> \cf3 LPsolTrainSet=matrix(rep(0,nUsers* nUsers),nrow= nUsers,ncol= nUsers)\cf4  \cf5 # Initialize matrix where values for the \cf3 nTrain\cf5  basic variables are dumped.\
#\
# Run the \cf3 nUsers \cf5 LPs with timer\
#\
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0
\cf3 >  start_time <- Sys.time();  \
for (j in 1:nUsers)\{lpOUT = lp ("min", rp3.obj[-j], TranslateDataForLP[,-j], rp3.dir, TranslateDataForLP[,j]); temp<- t(lpOUT$solution); temp=append(temp, -1.0*lpOUT$objval, after=(j-1)); LPsolTrainSet[ ,j]<-temp\}; \
end_time <- Sys.time()\
> LPtime=end_time - start_time; LPtime\
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0
\cf2 Time difference of 52.47267 secs.  #1st trial for Yahoo!Artists data (50 X 1235)\
Time difference of 53.05836 secs. #2nd trial for Yahoo!Artists\
Time difference of 31.31271 secs  MovieLens\
Time difference of 13.19595 secs for #1st CrowdFlower\
Time difference of 13.65908 secs for #2nd trial CrowdFlower\cf3 \
\
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0
\cf0 #\cf3 \
\cf0 # \ul Part 6: Predictions on the test data using the trained model:\
\ulnone # Step 1: Create the matrix of averges needed to make predictions. Req\'92d in \cf4 Expression (2.3) in JannachZankerFelfernigFriedrich11\cf0  \ul \
\ulnone # Step 2. Using the  Pearson\'92s correlations.\
# Step 3. Using the Convex Hulls.\
#\
\
#\
#\
# Step 1: Create the vector AVGS with the averages of each user\'92s ratings for the 64 jokes \
# in the training set. (Req\'92d in \cf4 Expression (2.3) in JannachZankerFelfernigFriedrich11\cf0 )\
#\
#\
# Move the data frame (which has annoying row and columns names) to a temporary file\
# so we can perform some numerical operations e.g. averages.\
#\
>\cf3 rm(temp)\cf0       # start with a new temp matrix.\
\cf3 >temp=as.matrix(t(trainMtx))\cf0    # Removes row names.\
> \cf3 dimnames(temp) <- NULL\cf0             # Removes column names.\
\
#\
#\
#\
> \cf3 AVGS=rowMeans(temp[])    \cf4 # The vector of averages\cf0 \
\
#\
# Calculate predictions for the \cf4 (\cf3 nItems\cf4 -\cf3 nTrain\cf4 )\cf0  items in the test/validation set using Pearson\'92s.\
# Start by initializing the matrix where the predictions will be placed.\
#\
\cf3 > PredictPearsonAll=matrix(rep(0,nUsers*(nItems-nTrain)),nrow=nUsers, ncol=(nItems-nTrain))  \cf4 # Initialize matrix.\
\
#\
# Calculate predictions for the (\cf3 nItems\cf4 -\cf3 nTrain\cf4 ) Artists in the test/validation set using formula Expression (2.3) in JannachZankerFelfernigFriedrich11. \
# Note: the formula uses neighbors based on the correlation \'93proximity\'94; i.e., the \cf3 nTrain\cf4  most correlated users.\
#\
\cf3 > for(i in 1:(nItems-nTrain))\{for (j in 1:nUsers)\{PredictPearsonAll[j,i]= AVGS[j]+(AllPearsCorr[order(AllPearsCorr[,j],decreasing=T)[2:(nItems-nTrain)+1],j]%*%(utilMtxT[order(AllPearsCorr[,j],decreasing=T)[2:(nItems-nTrain)+1],i+(nItems-nTrain)]-AVGS[order(AllPearsCorr[,j],decreasing=T)[2:(nItems-nTrain)+1]]))/sum(AllPearsCorr[order(AllPearsCorr[,j],decreasing=T)[2:(nItems-nTrain)+1],j])\}\};\
\
\cf4 #\
# Same instruction with timer\
#\cf3 \
start_time <- Sys.time(); \
for(i in 1:(nItems-nTrain))\{for (j in 1:nUsers)\{PredictPearsonAll[j,i]= AVGS[j]+(AllPearsCorr[order(AllPearsCorr[,j],decreasing=T)[2:(nItems-nTrain)+1],j]%*%(utilMtxT[order(AllPearsCorr[,j],decreasing=T)[2:(nItems-nTrain)+1],i+(nTrain)]-AVGS[order(AllPearsCorr[,j],decreasing=T)[2:(nItems-nTrain)+1]]))/sum(AllPearsCorr[order(AllPearsCorr[,j],decreasing=T)[2:(nItems-nTrain)+1],j])\}\}; end_time <- Sys.time(); CorrPredictTime=end_time - start_time;  CorrPredictTime\
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0
\cf2 Time difference of 15.70805 secs		\cf4 #For the Yahoo!Artists data set (50X1235).\cf3 \
\cf2 Time difference of 10.22912 secs    \cf4 #For the CrowdFlower data set (50X893).\
\cf2 Time difference of 10.0142 secs\cf4 	#For the CrowdFlower data set (50X893).\
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0
\cf7 Time difference of 1.662975 mins \cf4 #For the Jester data set (64X2911)\
\
#\
# Generate the predictions for the remaining (\cf3 nItems\cf4 -\cf3 nTrain\cf4 ) \'93new\'94 items in the test/validation set\
# using Convex Hulls\
\cf0 # Start by initializing the matrix where the predictions will be put.\cf4 \
#\cf0 \ul \
\cf3 \ulnone \
> PredictCHullsAll=matrix(rep(0,nUsers*(nItems-nTrain)),nrow=nUsers, ncol=(nItems-nTrain))\
\cf0 #\
\cf4 # Calculate predictions using formula Expression (2.3) in JannachZankerFelfernigFriedrich11. \
# Note: the formula uses neighbors based on the extreme points of the nearest projecting facet of the convex of the users\'92 data.\cf0 \
#\cf3 \
\
> for(i in 1:(nItems-nTrain))\{for (j in 1:nUsers)\{PredictCHullsAll[j,i]=AVGS[j]+(LPsolTrainSet[-j,j]%*%(utilMtxT[-j,i+nTrain]-AVGS[-j]))/(-1*LPsolTrainSet[j,j])\}\}\
\
\cf4 #\
# Same instruction with timer\
# \
\cf3 >start_time <- Sys.time(); for(i in 1:(nItems-nTrain))\{for (j in 1:nUsers)\{PredictCHullsAll[j,i]=AVGS[j]+(LPsolTrainSet[-j,j]%*%(utilMtxT[-j,i+nTrain]-AVGS[-j]))/(-1*LPsolTrainSet[j,j])\}\};end_time <- Sys.time(); ConvexHullPredictTime=end_time - start_time; ConvexHullPredictTime\
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0
\cf2 Time difference of 10.72478 mins MovieLens data set.\
Time difference of 10.82157 mins MovieLens data set.\cf3 \
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0
\cf4 #\
# Write the predictions using convex hulls into file "PredictCHullsAll.csv"\
# PredictCHullsAll\
#\
\
\cf3 > write.csv(PredictPearsonAll,"PredictPearsonAll.csv")}